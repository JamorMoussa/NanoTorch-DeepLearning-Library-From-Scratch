{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NanoTorch","text":""},{"location":"#what-is-nanotorch","title":"What is NanoTorch?","text":"<p>NanoTorch is a deep learning library inspired by the PyTorch framework, created using only Math and Numpy. It is designed for academic purposes, providing an educational resource for understanding the inner workings of neural networks.</p>"},{"location":"#why-do-we-need-nanotorch","title":"Why Do We Need NanoTorch?","text":"<p>NanoTorch is created for educational purposes, aiming to demystify the complexities of neural networks and their efficient implementation.</p>"},{"location":"#the-purpose-of-nanotorch","title":"The Purpose of NanoTorch","text":"<p>When learning about deep learning, we often encounter Neural Networks\u2014mathematical abstractions of the human brain composed of linear layers and activation functions (non-linearities). These networks learn complex mathematical functions by analyzing large datasets.</p> <p></p> <p>To build these networks, we typically use well-designed frameworks like PyTorch from Meta or TensorFlow from Google. However, several questions arise:</p> <ol> <li>How do these neural networks work behind the scenes?</li> <li>How do frameworks implement them so efficiently?</li> </ol>"},{"location":"#understanding-the-implementation","title":"Understanding the Implementation","text":"<p>While books explain the mathematics behind neural networks, their implementations often fall short of the efficiency and scalability seen in major frameworks. Understanding the source code of frameworks like PyTorch is challenging due to the additional layers of error handling and optimizations.</p>"},{"location":"#the-vision-of-nanotorch","title":"The Vision of NanoTorch","text":"<p>NanoTorch addresses this gap by providing a library with a PyTorch-like syntax and comprehensive documentation. This documentation not only explains the code but also delves into the mathematical concepts (e.g., gradients, matrices) and the process of transforming these equations into efficient and optimized code.</p>"},{"location":"#when-we-use-nanotorch","title":"When we use NanoTorch","text":"<p>Note</p> <p>NanoTorch is not recommended for real-world projects. It is not as powerful as PyTorch or TensorFlow and is intended solely for educational purposes.</p> <p>By understanding how NanoTorch works, you will gain familiarity with the implementation principles of frameworks like PyTorch, providing a strong foundation for further exploration in the field of deep learning.</p>"},{"location":"docs/tensor/","title":"Tensor","text":""},{"location":"docs/tensor/#whats-tensor","title":"What's Tensor?","text":"<p>Deep learning at a low level can be seen as just tensor manipulation. A tensor can be defined as a generalization of vectors and matrices to higher dimensions. Thus, tensors are at the heart of deep learning.</p>  Tensor  <p>The first thing we need to address in building <code>NanoTorch</code> is having a powerful and efficient tensors module that ensures numerical stability. Achieving this can be challenging and is not our primary mission here. Therefore, we're going to use <code>Numpy</code>, a Python library that provides powerful N-dimensional arrays. <code>Numpy</code> is fast (C implementation) and easy to use, making it an excellent choice for handling tensor operations.</p> <p>However, we need to make some decisions about the way of integrating the <code>Numpy</code> library to ensure compatibility with other modules in NanoTorch. We'll discuss these decisions in a few moments.</p>"},{"location":"docs/tensor/#numpy-for-numerical-operations","title":"Numpy for Numerical Operations","text":"<p>Numpy is a library based on <code>ndarray</code>, a multi-dimensional array of the same type. It offers algebraic operations in an efficient way. Let's look at some examples with Numpy and then explore how we can use it to build the <code>tensor</code> module for <code>NanoTorch</code>.</p> <pre><code>import numpy as np\n</code></pre> <pre><code>\u251c\u2500\u2500 nanotorch\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tensor\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ops.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 tensor.py\n</code></pre>"},{"location":"docs/tensor/tensor/","title":"Tensor","text":"<p>Deep learning at a low level can be seen as just tensor manipulation. A tensor can be defined as a generalization of vectors and matrices to higher dimensions. Thus, tensors are at the heart of deep learning.</p>  Tensor  <p>The first thing we need to address in building <code>NanoTorch</code> is having a powerful and efficient tensors module that ensures numerical stability. Achieving this can be challenging and is not our primary mission here. Therefore, we're going to use <code>Numpy</code>, a Python library that provides powerful N-dimensional arrays. <code>Numpy</code> is fast (C implementation) and easy to use, making it an excellent choice for handling tensor operations.</p> <p>However, we need to make some decisions about the way of integrating the <code>Numpy</code> library to ensure compatibility with other modules in NanoTorch. We'll discuss these decisions in a few moments.</p> <p>The tensor module contains two files, <code>tensor.py</code> that include implementing the <code>Tensor</code> class. The <code>ops.py</code> contains functions and operations such as <code>dot</code>, <code>sum</code>, etc.</p> <pre><code>\u251c\u2500\u2500 nanotorch\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tensor\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ops.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 tensor.py\n</code></pre> In\u00a0[\u00a0]: Copied! <pre>import numpy as np \nfrom typing import Self\n</pre> import numpy as np  from typing import Self In\u00a0[\u00a0]: Copied! <pre>class Tensor(np.ndarray):\n\n    def __new__(cls, input: Self) -&gt; Self:\n        \n        if isinstance(input, (int, float)): input = np.array([input,])\n        \n        if not isinstance(input, (Tensor, list, tuple, np.ndarray)): \n            raise ValueError(f\"the 'input' attributes must be list, tuple, numpy.ndarray. But '{input.__class__.__name__}' is given\") \n        \n        # reshape to 2-d if the input is 1-d:\n        if not isinstance(input, np.ndarray): input = np.array(input)\n        if input.ndim == 1: input = input.reshape(1, -1)\n        \n        # create a view : \n        obj = np.asanyarray(input).view(cls)\n\n        return obj\n</pre> class Tensor(np.ndarray):      def __new__(cls, input: Self) -&gt; Self:                  if isinstance(input, (int, float)): input = np.array([input,])                  if not isinstance(input, (Tensor, list, tuple, np.ndarray)):              raise ValueError(f\"the 'input' attributes must be list, tuple, numpy.ndarray. But '{input.__class__.__name__}' is given\")                   # reshape to 2-d if the input is 1-d:         if not isinstance(input, np.ndarray): input = np.array(input)         if input.ndim == 1: input = input.reshape(1, -1)                  # create a view :          obj = np.asanyarray(input).view(cls)          return obj"},{"location":"docs/tensor/tensor/#tensor","title":"Tensor\u00b6","text":""},{"location":"docs/tensor/tensor/#whats-tensor","title":"What's Tensor?\u00b6","text":""},{"location":"docs/tensor/tensor/#the-structure-of-tensor-module","title":"The Structure of <code>Tensor</code> Module\u00b6","text":""},{"location":"docs/tensor/tensor/#tensor-class","title":"<code>Tensor</code> class\u00b6","text":""}]}